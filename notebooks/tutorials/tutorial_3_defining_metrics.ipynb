{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ConstantKernel, Matern\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.chdir(\"../../\")\n",
    "\n",
    "plt.style.use(\"src/matplotlib.rc\")\n",
    "warnings.filterwarnings('ignore') \n",
    "np.random.seed(47)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading (same as tutorial 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/usnistgov/remi/raw/nist-pages/data/Combinatorial%20Libraries/Fe-Co-Ni/FeCoNi_benchmark_dataset_220501a.mat\n",
    "!mkdir datasets\n",
    "!mv FeCoNi_benchmark_dataset_220501a.mat datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_ternary_data\n",
    "\n",
    "X, Y = load_ternary_data('datasets/FeCoNi_benchmark_dataset_220501a.mat')\n",
    "n_features, n_properties = X.shape[1], Y.shape[1]\n",
    "\n",
    "x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "scalers = [x_scaler, y_scaler]\n",
    "\n",
    "X_norm = x_scaler.fit_transform(X)\n",
    "Y_norm = y_scaler.fit_transform(Y)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm definition (same as tutorial 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.algorithms import SubsetAlgorithm\n",
    "\n",
    "class PercentileIntersection2D(SubsetAlgorithm):\n",
    "    def __init__(self, user_algo_params):\n",
    "        super().__init__(user_algo_params)\n",
    "\n",
    "    def user_algorithm(self, f_x, x):\n",
    "        # percentile threshold value for property 1 \n",
    "        percentile_list = self.user_algo_params['percentile_list']\n",
    "        percentile_threshold_p1 = np.percentile(f_x[:, 0], percentile_list[0])\n",
    "\n",
    "        # percentile threshold value for property 2    \n",
    "        percentile_threshold_p2 = np.percentile(f_x[:, 1], percentile_list[1])\n",
    "\n",
    "        # Determine the ids where each condition holds seperately \n",
    "        ids1 = set(np.where(f_x[:,0] >= percentile_threshold_p1)[0])\n",
    "        ids2 = set(np.where(f_x[:,1] >= percentile_threshold_p2)[0])\n",
    "\n",
    "        # Take the union to express the \"either/or\" logic. \n",
    "        return list(ids1.union(ids2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the $\\mathsf{Number}$ $\\mathsf{Obtained}$ metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\mathsf{Number}$ $\\mathsf{Obtained}$ metric quanties how many true target ids have been measured. (Or equivalently, how many measurements actually satisfy the experimental goal.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 537, 538, 25, 26, 27, 28, 29, 721, 722, 566, 567, 56, 57, 58, 59, 60, 61, 62, 64, 65, 67, 68, 72, 74, 75, 595, 84, 596, 597, 98, 99, 100, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 622, 623, 624, 125, 128, 648, 649, 650, 651, 652, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 675, 173, 677, 696, 697, 698, 699, 700, 701, 198, 199, 200, 201, 202, 203, 204, 205, 206, 719, 720, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 723, 724, 740, 741, 742, 743, 744, 760, 761, 762, 763, 764, 765, 779, 780, 781, 782, 783, 784, 797, 798, 799, 800, 801, 802, 814, 815, 816, 817, 818, 673, 829, 830, 831, 674, 832, 833, 676, 843, 844, 845, 846, 847, 856, 857, 858, 859, 868, 869, 870, 871, 879, 880, 881, 889, 890, 897, 898, 904, 905, 910, 914, 915, 918, 920, 506]\n"
     ]
    }
   ],
   "source": [
    "user_algo_params = {'scalers': scalers, 'percentile_list': [90, 90]}\n",
    "algorithm = PercentileIntersection2D(user_algo_params)\n",
    "\n",
    "# true target ids (ground_truth)\n",
    "target_subset_ids = algorithm.identify_subspace(f_x = Y_norm, x = X_norm)\n",
    "print(target_subset_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's assume that we have made <code>n_data</code> measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[517 236 507 358 907 347 315 520 805 853 533 713   7 654 435 616 485 329\n",
      "  75 505 455 561 689 645 261  33 296 461 488 872  70 587  79 453 196 400\n",
      " 371 166 583 530 808 844 518  20 241 157 475 272 383 648]\n"
     ]
    }
   ],
   "source": [
    "n_data = 50 # max is X.shape[1]\n",
    "collected_ids = np.random.choice(np.arange(0, X_norm.shape[0] + 1), size=n_data, replace=False)\n",
    "print(collected_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_obtained(collected_ids: list, true_target_ids: list) -> int:\n",
    "    n_obtained = len(set(collected_ids).intersection(set(true_target_ids)))\n",
    "    return n_obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_obtained(collected_ids=collected_ids, true_target_ids=target_subset_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the above number is low. This is because we chose datapoints via random sampling. We aim to do significantly better using a smart acquisition approach (see tutorial notebook 3). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the $\\mathsf{Posterior}$ $\\mathsf{Jaccard}$ $\\mathsf{Index}$ metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Posterior Jaccard Index quantifies how well the MODEL understands the location/shape of the true target subset. To begin, let's train a simple GP model based on a small amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import MGPR\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, Y_norm, test_size=0.975, random_state=42)\n",
    "X_train.shape, y_train.shape\n",
    "\n",
    "kernel_initial = ConstantKernel(constant_value=1.0, constant_value_bounds=[0.01, 3.0]) * Matern(nu = 5/2, length_scale= n_features * [1.0], length_scale_bounds= n_features * [[0.01, 3.0]]) + WhiteKernel(noise_level=0.01, noise_level_bounds='fixed')\n",
    "kernel_initial_list = n_properties * [kernel_initial]\n",
    "multi_gpr = MGPR(kernel_list=kernel_initial_list)\n",
    "multi_gpr.fit(X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can take the posterior mean, $\\bar{f}$ as the overall prediction of the true function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mean, posterior_std = multi_gpr.predict(X_norm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, this metric looks at the difference in the ground truth set and the predicted set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 537, 538, 25, 26, 27, 28, 29, 721, 722, 566, 567, 56, 57, 58, 59, 60, 61, 62, 64, 65, 67, 68, 72, 74, 75, 595, 84, 596, 597, 98, 99, 100, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 622, 623, 624, 125, 128, 648, 649, 650, 651, 652, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 675, 173, 677, 696, 697, 698, 699, 700, 701, 198, 199, 200, 201, 202, 203, 204, 205, 206, 719, 720, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 723, 724, 740, 741, 742, 743, 744, 760, 761, 762, 763, 764, 765, 779, 780, 781, 782, 783, 784, 797, 798, 799, 800, 801, 802, 814, 815, 816, 817, 818, 673, 829, 830, 831, 674, 832, 833, 676, 843, 844, 845, 846, 847, 856, 857, 858, 859, 868, 869, 870, 871, 879, 880, 881, 889, 890, 897, 898, 904, 905, 910, 914, 915, 918, 920, 506]\n",
      " \n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 652, 537, 538, 539, 540, 541, 542, 722, 566, 567, 568, 569, 570, 571, 572, 595, 596, 597, 598, 599, 600, 622, 623, 624, 625, 626, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 719, 720, 721, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 740, 741, 742, 760, 761, 762, 763, 779, 780, 781, 797, 798, 799, 814, 815, 816, 673, 829, 830, 674, 675, 676, 843, 844, 845, 677, 856, 857, 868, 869, 870, 879, 880, 889, 890, 897, 898, 904, 905, 910, 914, 696, 648, 697, 698, 441, 442, 443, 699, 700, 649, 474, 475, 476, 477, 478, 650, 651, 506, 507, 508, 509, 510, 511]\n"
     ]
    }
   ],
   "source": [
    "user_algo_params = {'scalers': scalers, 'percentile_list': [90, 90]}\n",
    "algorithm = PercentileIntersection2D(user_algo_params)\n",
    "\n",
    "# true target ids (ground_truth)\n",
    "target_subset_ids = algorithm.identify_subspace(f_x = Y_norm, x = X_norm)\n",
    "print(target_subset_ids)\n",
    "\n",
    "print(\" \")\n",
    "# predicted target ids based on posterior mean function\n",
    "posterior_mean_ids = algorithm.identify_subspace(f_x = posterior_mean, x = X_norm)\n",
    "print(posterior_mean_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jaccard Posterior Index measures the intersection/union of these two sets. This is a metric between 0.0 and 1.0 which measures set overlap; here, 1.0 means the two sets are identical (i.e. a perfect model in the target region of the design space) and 0.0 means the two sets are disjoint (a terrible model in target space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard_posterior(predicted_target_ids: list, true_target_ids: list) -> float:\n",
    "    intersection = len(set(predicted_target_ids).intersection(set(true_target_ids)))\n",
    "    union = len(set(predicted_target_ids).union(set(true_target_ids)))\n",
    "    jaccard =  intersection/union \n",
    "    return jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4144486692015209"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_jaccard_posterior(predicted_target_ids = posterior_mean_ids, true_target_ids = target_subset_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please check out [tutorial_4](tutorial_4_data_acquisition_using_BAX.ipynb) for an example of data acquisition using BAX."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (default, Jul  2 2020, 11:26:31) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89b5c5be7d9a5b6c117ecb4d0f9593b952cdb99d9e5b93dbe620fac69234b982"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
