{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "import os \n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ConstantKernel, Matern, RBF\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "os.chdir(\"../\")\n",
    "plt.style.use(\"mpl_style/matplotlib.rc\")\n",
    "\n",
    "from src.utils import XY_from_csv, random_sampling_no_replace\n",
    "from src.acquisition import run_acquisition\n",
    "from src.algorithms import MultibandUnion, MultibandUnionIntersection, GlobalOptimization1D, ParetoFront, Wishlist, PercentileSet1D, MonodisperseLibrary\n",
    "from src.models import MGPR, fit_hypers\n",
    "from src.metrics import get_n_obtained, get_jaccard_posterior\n",
    "from src.plotting import plot_final_metrics, plot_iteration_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_y_range = [[0, 30], [0,30]]\n",
    "\n",
    "X, Y = XY_from_csv(\"datasets/np_synthesis.csv\", columns_x=[\"c1\", \"c2\", \"pH\", \"T\"], columns_y=[\"size\", \"dispersity\"])\n",
    "\n",
    "x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "y_scaler.fit(estimated_y_range)\n",
    "scalers = [x_scaler, y_scaler]\n",
    "\n",
    "X = x_scaler.fit_transform(X)\n",
    "Y = y_scaler.fit_transform(Y)\n",
    "\n",
    "n_features = X.shape[1]\n",
    "n_properties = Y.shape[1]\n",
    "\n",
    "# handles one-property measurements\n",
    "if len(Y.shape) == 1:\n",
    "    Y = Y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm = MultiRegionSetIntersection(threshold_list = [[0.0, 20.0], [0.0, 2.5]], scalers = scalers)\n",
    "\n",
    "user_algo_params = {'scalers': scalers,\n",
    "                    'target_radii_list': [6, 8,  12, 14, 16, 18, 20, 25],\n",
    "                    'polysdispersity_threshold': 5.0,\n",
    "                    'target_radii_tol':0.5}\n",
    "\n",
    "algorithm = MonodisperseLibrary(user_algo_params=user_algo_params)\n",
    "\n",
    "\n",
    "\n",
    "# algorithm = Wishlist(\n",
    "#     threshold_bounds=[[[0.0, 2.0], [0.0, 0.2]], [[3.0, 4.0], [0.3, 0.4]], [[8.0, 10.0], [0.0, 0.1]]], scalers=scalers\n",
    "# )\n",
    "\n",
    "# algorithm = PercentileSet(percentile_threshold=95, scalers=scalers)\n",
    "# algorithm = ParetoFront(tolerance_list = [1.0, 1.0],  max_or_min_list = [0, 0], scalers = scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = list(np.arange(0, len(X)))  # integer mapping design space\n",
    "true_target_ids = algorithm.identify_subspace(\n",
    "    f_x=Y, x=X\n",
    ")  # ground truth set of design points which achieve experimental goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting = True\n",
    "prevent_requery = True\n",
    "plot_frequency = 50\n",
    "n_posterior_samples = 15 # relevant for InfoBAX and mixedBAX \n",
    "n_initial = 1 # Number of initial datapoints \n",
    "n_iters = 201 # Number of measurements to be performed \n",
    "n_repeats = 1 # Repeats with different dataset initializations \n",
    "fixed_hypers = False \n",
    "adaptive_fit_freq = 10\n",
    "\n",
    "kernel_initial = ConstantKernel(constant_value=1.0, constant_value_bounds=[0.01, 3.0]) * Matern(nu = 5/2, length_scale= n_features * [1.0], length_scale_bounds= n_features * [[0.01, 3.0]]) + WhiteKernel(noise_level=0.01, noise_level_bounds='fixed')\n",
    "\n",
    "kernel_initial_list = n_properties * [kernel_initial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 17/201 [00:03<00:31,  5.88it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = {\n",
    "    \"SwitchBAX\": {\"n_obtained\": [], \"jaccard_posterior_index\": [], \"switch_strategy\": []},\n",
    "    \"US\": {\"n_obtained\": [], \"jaccard_posterior_index\": [], \"switch_strategy\": []},\n",
    "    \"MeanBAX\": {\"n_obtained\": [], \"jaccard_posterior_index\": [], \"switch_strategy\": []},\n",
    "    \"InfoBAX\": {\"n_obtained\": [], \"jaccard_posterior_index\": [], \"switch_strategy\": []},\n",
    "}\n",
    "\n",
    "# Baseling for random sampling without replacement (expectation of hypergeometric distribution)\n",
    "random_sampling = [random_sampling_no_replace(len(X), len(true_target_ids), n) for n in range(n_initial, n_iters)]\n",
    "\n",
    "# Baseline for best possible acquisition (i.e. acquire a target point at each iteration; need an \"oracle\" to do this)\n",
    "if n_iters <= len(true_target_ids):\n",
    "    best_possible_n_obtained = np.arange(n_initial, n_iters + n_initial)\n",
    "else:\n",
    "    best_possible_n_obtained = list(np.arange(n_initial, len(true_target_ids))) + list(\n",
    "        len(true_target_ids) * np.ones(n_iters + n_initial - len(true_target_ids))\n",
    "    )\n",
    "\n",
    "# Acquisition functions that use BAX for subset estimation\n",
    "# strategies = [\"MeanBAX\", \"SwitchBAX\", \"US\", \"InfoBAX\"]\n",
    "strategies = [\"MeanBAX\", \"SwitchBAX\", \"US\"]\n",
    "\n",
    "# Calculate hypers based on the entire dataset; this is not possible in a real experiment but allows us to compare acquisition fn to acquisition fn\n",
    "if fixed_hypers:\n",
    "    kernel_list = fit_hypers(x_train=X, y_train=Y, kernel_list=kernel_initial_list, n_restarts_optimizer=1)\n",
    "\n",
    "for strategy in strategies:\n",
    "    for j in range(n_repeats):  # to see variance w.r.t initial datapoint choice\n",
    "        np.random.seed(j) # make sure all strategies get same initial points\n",
    "        train_indices = list(np.random.choice(all_ids, n_initial))\n",
    "        x_train = X[train_indices]\n",
    "        y_train = Y[train_indices]\n",
    "\n",
    "        collected_ids = list(train_indices)\n",
    "        n_obtained_list = []\n",
    "        jaccard_posterior_list = []\n",
    "        switch_list = [] \n",
    "\n",
    "        for i in tqdm(range(n_iters)):\n",
    "            # Adaptive hyperparameter fitting\n",
    "            if (i % adaptive_fit_freq == 0) and (fixed_hypers == False):\n",
    "                kernel_list = fit_hypers(x_train=x_train, y_train=y_train, kernel_list=kernel_initial_list)\n",
    "            \n",
    "            # Define GP model with fixed, fitted hypers. Note, we need this so that all the n_posterior models for InfoBAX have the same kernel\n",
    "            multi_gpr = MGPR(kernel_list=kernel_list)\n",
    "\n",
    "            # Acquire next index\n",
    "            x_train, y_train, model, collected_ids, acquisition_function, switch_strategy = run_acquisition(\n",
    "                x_train, y_train, X, Y, strategy, algorithm, multi_gpr, collected_ids, n_posterior_samples\n",
    "            )\n",
    "\n",
    "            # Calculate metrics\n",
    "            posterior_mean, posterior_std = model.predict(X)\n",
    "            predicted_target_ids = algorithm.identify_subspace(f_x=posterior_mean, x=X)\n",
    "            n_obtained_list.append(get_n_obtained(collected_ids, true_target_ids))\n",
    "            jaccard_posterior_list.append(get_jaccard_posterior(predicted_target_ids, true_target_ids))\n",
    "            switch_list.append(switch_strategy)\n",
    "\n",
    "            if (i % plot_frequency == 0) and (plotting) and (i != 0):\n",
    "                plot_iteration_results(\n",
    "                    X,\n",
    "                    Y,\n",
    "                    x_scaler,\n",
    "                    y_scaler,\n",
    "                    collected_ids,\n",
    "                    true_target_ids,\n",
    "                    predicted_target_ids,\n",
    "                    acquisition_function,\n",
    "                    n_obtained_list,\n",
    "                    jaccard_posterior_list,\n",
    "                    best_possible_n_obtained,\n",
    "                    random_sampling,\n",
    "                    n_initial\n",
    "                )\n",
    "\n",
    "        metrics[strategy][\"n_obtained\"].append(n_obtained_list)\n",
    "        metrics[strategy][\"jaccard_posterior_index\"].append(jaccard_posterior_list)\n",
    "        metrics[strategy][\"switch_strategy\"].append(switch_list)\n",
    "\n",
    "        plt.figure(figsize=(10,3))\n",
    "        plt.step(np.arange(n_initial, n_initial + n_iters), metrics[strategy]['switch_strategy'][0])\n",
    "        plt.ylabel('Exploit/Explore')\n",
    "        plt.xlabel('Dataset Size')\n",
    "        plt.show()\n",
    "\n",
    "plot_final_metrics(n_iters, metrics, strategies, best_possible_n_obtained, random_sampling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "89b5c5be7d9a5b6c117ecb4d0f9593b952cdb99d9e5b93dbe620fac69234b982"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
